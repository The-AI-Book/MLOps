{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 2. Quantization & Pruning\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mobile, IoT, and Similar Use Cases\n",
    "---\n",
    "- Trends in adoption of smart devices\n",
    "- Demands move ML capability from cloud to on-device\n",
    "- Cost-effectiveness\n",
    "- Compliance with privacy regulations\n",
    "\n",
    "\n",
    "### Online ML inference\n",
    "---\n",
    "1. To generate real-time predictions you can:\n",
    "- Host the model on a server\n",
    "- Embed the model in the device\n",
    "\n",
    "2. Is it faster on a server, or on-device?\n",
    "3. Mobile processing limitations?\n",
    "\n",
    "<img src = \"https://i.gyazo.com/2cf8354b9ad0d99dfd7e03ded86534f2.png\" width = \"500px\">\n",
    "<img src = \"https://i.gyazo.com/f5fd34b88e8b5d4c19c31ba56ecbf68c.png\" width = \"500px\">\n",
    "\n",
    "### Model deployment\n",
    "\n",
    "<img src = \"https://i.gyazo.com/8033e6966b9ef0818f0513ca2dbe30d1.png\" width = \"500px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benefits and Process of Quantization\n",
    "---\n",
    "\n",
    "- Quantization involves transforming a model into an equivalent representation that uses parameters and computations at a lower precision. This improves the model's execution performance and efficiency, but it can often result in lower model accuracy.\n",
    "- Quantization, in essence, lessens or reduces the number of bits needed to represent information. However, you may notice that as you reduce the number of pixels beyond a certain point, depending on the image, it may get harder to recognize what that image is.\n",
    "\n",
    "### Why quantize neural networks?\n",
    "- Neural networks have many parameters and take up space\n",
    "- Srinking model file size\n",
    "- Reduce computational resources\n",
    "- Make models run faster and use less power with low-precision\n",
    "\n",
    "### Benefits of quantization\n",
    "- Faster compute\n",
    "- Low memory bandwidth\n",
    "- Low power\n",
    "- Integer operations supported across CPU/DSP/NPUs\n",
    "\n",
    "### The quantization process\n",
    "<img src = \"https://i.gyazo.com/16553809c1015c2cb41ea2d4782e68c0.png\" width = \"500px\">\n",
    "\n",
    "### What parts of the model are affected?\n",
    "- Static values (parameters)\n",
    "- Dynamic values (activations)\n",
    "- Computation (transformations)\n",
    "\n",
    "### Trade-offs\n",
    "- Optimization impacts model accuracy: Difficult to predict ahead of time\n",
    "- In rare cases, models may actually gain some accuracy\n",
    "- Undefined effects on ML interpretability\n",
    "\n",
    "\n",
    "### Chose the best model for the task\n",
    "- Trade-off between model accuracy and model complexity\n",
    "\n",
    "## Post Training Quantization\n",
    "\n",
    "---\n",
    "\n",
    "- Post-training quantization is a conversion technique that can reduce model size while also improving CPU and hardware accelerator latency with little degradation in model accuracy. \n",
    "- You can quantize an already trained TensorFlow model when you convert it to TensorFlow Lite format using the TensorFlow Lite converter.\n",
    "-  What post-training quantization basically does is to convert, or more precisely, quantize the weights from floating point numbers to integers in an efficient way. By doing this, you can gain up to three times lower latency without taking a major hit on accuracy. With the default optimization strategy, the converter will do its best to apply a post-training quantization, trying to optimize the model for both size and latency. \n",
    "\n",
    "<img src = \"https://i.gyazo.com/2d37ff842c9d1c49584527d0de3b1ccf.png\" width = \"500px\">\n",
    "\n",
    "\n",
    "- Using dynamic range quantization, you can reduce the model size and/or latency, but this comes with a limitation as it requires inference to be done with floating point numbers. \n",
    "- This may not always be ideal since some hardware accelerators only support integer operations, for example, Edge TPUs. The optimization toolkit also supports post-training integer quantization. \n",
    "- This enables users to take an already trained floating point model and fully quantize it to use only eight bits signed integer, which enables fixed point hardware accelerators to run these models. \n",
    "- When targeting greater CPU improvements or fixed point accelerators, this is often a better option. Post-training integer quantization works by gathering calibration data, which it does by running inferences on a small set of inputs so as to determine the right scaling parameters needed to convert the model to an integer quantized model. \n",
    "- Post-training quantization can result in a loss of accuracy, particularly for smaller networks, but it is often fairly negligible. On the plus side, this will speed up execution of the heaviest computations by using lower precision and the most sensitive computations with higher precision, thus typically resulting in little or no final loss of accuracy. \n",
    "\n",
    "### Model accuracy\n",
    "\n",
    "- Small accuracy loss incurred (mostly for smaller networks).\n",
    "\n",
    "<img src = \"https://i.gyazo.com/8216f7ab4f8b739b2dac77e1e4da870d.png\" width = \"500px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantization Aware Training\n",
    "---\n",
    "\n",
    "- Inserts fake quantization (FQ) nodes in the forward pass\n",
    "- Rewrites the graph to emulate quantized inference\n",
    "- Reduce the loss of accuracy due to quantization\n",
    "- Resulting model contains all data to be quantized according to spec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pruning\n",
    "\n",
    "---\n",
    "\n",
    "<img src = \"https://i.gyazo.com/6af44724ac31c70cbe173b5cb83ff837.png\" width = \"500px\">\n",
    "\n",
    "- Pruning aims to reduce the number of parameters and operations involved in generating a prediction by removing network connections. \n",
    "- With pruning, you can lower the overall parameter count in the network. Networks generally look like the one on the left.\n",
    "- Here every neuron in a layer has a connection to the layer before it, but this means we have to multiply a lot of floats together.\n",
    "-  Ideally, we'd only connect each neuron to a few others and save on doing some of the multiplications, if we can find a way to do that without too much loss of accuracy.\n",
    "- Restricting the search space **can also act as a regularizer**.\n",
    "- Better storage and/or transmission\n",
    "- Gain speedups in CPU and some ML accelerators\n",
    "- Can be used in tandem with quantization to get additional benefits\n",
    "- Unlock performace improvements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
