{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 5. Interpretability\n",
    "---\n",
    "\n",
    "<img src = \"https://i.gyazo.com/0cea903609c6cd754ba9e447cd9df1c1.png\">\n",
    "\n",
    "- Practically speaking, however, the level of effort required needs to be feasible as well. One measure of interpretability of models is the amount of effort or analysis required to understand a given result.\n",
    "\n",
    "<img src = \"https://i.gyazo.com/635e9647764998087c8711f5939c4fe3.png\">\n",
    "\n",
    "<img src = \"https://i.gyazo.com/b1e1c2bfdc1695d04f372a030f551f29.png\">\n",
    "\n",
    "- One way of grouping model interpretability methods is by whether the model itself is intrinsically interpretable. Model architectures that are intrinsically interpretable have been around for a long time. The classic examples of this are linear models and tree-based models.\n",
    "\n",
    "<img src = \"https://i.gyazo.com/8fdf30c4f361a733447652f92e184549.png\">\n",
    "\n",
    "### Intrinsic or Post-Hoc?\n",
    "\n",
    "<img src = \"https://i.gyazo.com/6e3c0f3b491b29fc1227e9cdc07c82f0.png\">\n",
    "\n",
    "- Post-hoc methods don't evaluate the actual sequence of operations that led to the generation of the results. In general, an intrinsically interpretable model provides a higher degree of certainty as to why it generated a particular result. Examples of these analyses include feature importance and partial dependency plots. \n",
    "\n",
    "<img src = \"https://i.gyazo.com/15a2f0980a8e4ca1164b550ca08a1262.png\">\n",
    "\n",
    "- For example, in case of less complex models such as linear models, you can look at the learned weights to produce an interpretation.\n",
    "- Similarly, the learned tree structure in a tree-based model serves as an interpretation. \n",
    "- In lattice models, the parameters of each layer are the output of that layer, which makes it relatively easy to analyze, understand, and debug each part of the model. \n",
    "- Some methods examine particular data points. One such method is counterfactual explanations. Counterfactual explanations are used to explain the prediction of a data point. - In order to do so, it finds another data point by changing some features so that the predicted output changes in a relevant way. The change should be significant. \n",
    "- For example, the new data point should be predicting a different class.\n",
    "\n",
    "<img src = \"https://i.gyazo.com/097d5246d782ca648a7366a5ed723779.png\">\n",
    "\n",
    "### Local or Global?\n",
    "- **Local:** interpretation method explains and individual prediction\n",
    "- Feature attribution is identification of relevant features as an explnation for a model\n",
    "- **Global:** interpretation methods explains entire model behaviour\n",
    "- Feature attribution summary for the entire test data set\n",
    "\n",
    "## Intrinsically Interpretable Models\n",
    "- How the model works is self evident\n",
    "- Many classic models are highly interpretable\n",
    "- Neural networks look like \"black boxes\"\n",
    "- Newer architectures focus on designing for interpretability\n",
    "\n",
    "### Monotonicity improves intepretability\n",
    "\n",
    "<img src = \"https://i.gyazo.com/4bc1508a5ed37b5cb71aa15c865c6d7b.png\" width = \"500px\">\n",
    "\n",
    "- This matches your knowledge of the world and, so your model should match it too and the mileage feature should be monotonic. In this graph, the blue and green curves are monotonic, while the red curve is not because it does not consistently increase or decrease or remain the same.\n",
    "\n",
    "<img src = \"https://i.gyazo.com/2bf54a6c374371674f8828acc2ae7510.png\" width = \"500px\">\n",
    "<img src = \"https://i.gyazo.com/69d6ee18e85716de9821e22f1a304fe4.png\" width = \"500px\">\n",
    "<img src = \"https://i.gyazo.com/0eef1094ddb25dd76043c6accbdc37f1.png\" width = \"500px\">\n",
    "\n",
    "### Tensorflow Lattice\n",
    "\n",
    "<img src = \"https://i.gyazo.com/7135ae923f240749906c1a5a73055c5a.png\" width = \"500px\">\n",
    "<img src = \"https://i.gyazo.com/cbf4eff0e0cd2ba5757d1dd8d6746d32.png\" width = \"500px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
