{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 3. High-Performance Modeling\n",
    "---\n",
    "\n",
    "## Distributed Training\n",
    "\n",
    "---\n",
    "- At first, training models is quick and easy\n",
    "- Training models becomes more time-consuming: with more data, with larger models\n",
    "- Longer training, more epochs, less eficient\n",
    "- Use distributed training approaches\n",
    "\n",
    "### Types of distributed training\n",
    "- **Data parallelism:** models are replicated differente accelerators (GPU/TPU) and data is split between them\n",
    "\n",
    "- **Model parallelsim:** When models are too large to fit on a single device then they can be divided into partitions, assigning differente partitions to different accelerators\n",
    "\n",
    "<img src = \"https://i.gyazo.com/f50ba65245767fa1937dd9acfa2ee378.png\">\n",
    "\n",
    "- Each worker independently computes the errors between its predictions for its training samples and the labeled data. \n",
    "- Then each worker performs backprop to update its model based on the errors, and communicates all of its changes to the other workers so that they can update their models. \n",
    "- This means that the workers need to synchronize their gradients at the end of each batch to ensure that they are training a consistent model.\n",
    "\n",
    "<img src = \"https://i.gyazo.com/78520849bffd29aea556fde68c287a78.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making your models distribute-aware\n",
    "\n",
    "If you want to distribute a model:\n",
    "- Supported in high-level APIs such as Keras/Estimators\n",
    "- For more control, you can use custom training loops\n",
    "\n",
    "<img src = \"https://i.gyazo.com/8e205796c381207b7cf04c25886989fa.png\">\n",
    "\n",
    "<img src = \"https://i.gyazo.com/f3712d1e09757024b3179afbeeb45f84.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## High-Performance Ingestion\n",
    "---\n",
    "- Accelerators are a key part of high-performance modeling, training, and inference, but accelerators are also expensive, so it's important to use them efficiently. \n",
    "\n",
    "### Why input pipelines?\n",
    "- Data at times can't fit into memory and sometimes, CPUs are under-utilized in compute intensive tasks like trainign a complex model\n",
    "- You should avoid these inefficiencies so that you can make the most of hardware available, use pipelines\n",
    "\n",
    "### tf.data: TensorFlow Input Pipeline\n",
    "- You can view input pipelines as an ETL process, providing a framework to facilitate applying performance optimization. \n",
    "\n",
    "<img src = \"https://i.gyazo.com/1c40c596478f3d4f3ceadff2a7502d99.png\">\n",
    "\n",
    "### Inefficient ETL process\n",
    "\n",
    "<img src = \"https://i.gyazo.com/ff6dc4052d26825a5e6e4f07c7f5f146.png\">\n",
    "\n",
    "### An improved ETL process\n",
    "\n",
    "<img src = \"https://i.gyazo.com/0d1d5a4c6a29b695e436f56029239150.png\">\n",
    "\n",
    "- We reduce the time when the disk and CPU remain idle\n",
    "- Accelerator 100% utilized\n",
    "\n",
    "### Pipelining\n",
    "\n",
    "<img src = \"https://i.gyazo.com/2711da8b903a692965827da2fbdb4843.png\">\n",
    "\n",
    "### How to optimize pipeline performance?\n",
    "- Prefetching: where you begin loading data for the next step before the current step completes. \n",
    "- Parallelize data extraction and transformation\n",
    "- Caching: Caching the dataset to get started with training immediately once a new epic begins, is also very effective when you have enough cash. \n",
    "- Reduce memory\n",
    "\n",
    "### Parallelize data transformation\n",
    "<img src = \"https://i.gyazo.com/c38d10057a4b7b6cc9a6c8297dc177e0.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Large Models - The Rise of Giant Nueral Nets and Parallelism \n",
    "---\n",
    "\n",
    "## Overcoming memory constraints\n",
    "---\n",
    "\n",
    "- Strategy #1 - Gradient Accumulation: Split batches into mini-batches and only perform backprop after whole batch\n",
    "\n",
    "- Strategy #2 - Memory swap: Copy activation between CPU and memory, back and forth\n",
    "\n",
    "### Parallelism revisited\n",
    "\n",
    "<img src = \"https://i.gyazo.com/bc5c3d216270e6a492fe1e83670faa53.png\">\n",
    "\n",
    "### Challenges keeping accelerators busy\n",
    "\n",
    "<img src = \"https://i.gyazo.com/3e9807f42850590dd36e6eb21bee5f7a.png\" width = \"400px\">\n",
    "\n",
    "## Pipeline parallelism\n",
    "\n",
    "<img src = \"https://i.gyazo.com/cba36f24a34fc1d6ba44b84537c40c46.png\">\n",
    "\n",
    "- GPipe\n",
    "- Pipedream\n",
    "\n",
    "Integrates both data and model parallelism:\n",
    "- Divide mini-batch data into micro-batches\n",
    "- Differente workers work on different micro-batches in parallel\n",
    "- Allow ML models to have significantly more parameters\n",
    "\n",
    "### GPipe - Key features\n",
    "- Open-sources TensorFlow library (using Lingvo)\n",
    "- Inserts communication primiives at the partition boundaries\n",
    "- Automatic parallelism to reduce memory consumption\n",
    "- Gradient accumulation across micro-batches, so that model quality is preserved\n",
    "- Partitioning is heuristic-based\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
